{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Final Scraper"
      ],
      "metadata": {
        "id": "B4fw76-1-EiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJXDNMzxvURq",
        "outputId": "30d8fa79-892d-486c-ec2f-27425b3ad1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UniProtDatabase: 100%|██████████| 5/5 [00:06<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uniprot: wrote uniprot.fasta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "CDDDatabase: 100%|██████████| 5/5 [00:05<00:00,  1.15s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cdd: wrote cdd.fasta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PDBDatabase: 100%|██████████| 5/5 [00:03<00:00,  1.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pdb: wrote pdb.fasta\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#=============================================================Import all necessary libraries here=============================================================================================\n",
        "\n",
        "import time, requests,os\n",
        "from abc import ABC, abstractmethod\n",
        "from xml.etree import ElementTree as ET\n",
        "from tqdm import tqdm\n",
        "\n",
        "#======================================================================The Abstract Interface=====================================================================================================\n",
        "\n",
        "# This is the class for any new database any future scientist wants to use. We must define what we expect from the new database, and each database provides data in its own way.\n",
        "# There is no one size fits all. But this is the structure any future class must use\n",
        "\n",
        "#===============================================================================================================================================================================================\n",
        "\n",
        "class ProteinDatabase(ABC):\n",
        "    @abstractmethod\n",
        "    def search_accessions(self, domain, organism_id=None, max_proteins=None): ...\n",
        "    @abstractmethod\n",
        "    def fetch_fasta(self, accession): ...\n",
        "\n",
        "#======================================================================Implementing the Uniprot Version==========================================================================================\n",
        "\n",
        "# Uniprot allows for organism ID.\n",
        "\n",
        "#================================================================================================================================================================================================\n",
        "class UniProtDatabase(ProteinDatabase):\n",
        "    SEARCH = \"https://rest.uniprot.org/uniprotkb/search\" # The search url the function follows\n",
        "    FASTA  = \"https://rest.uniprot.org/uniprotkb/\" # The url to find Fasta at\n",
        "\n",
        "    def search_accessions(self, domain, organism_id=None, max_proteins=None): # setting the parameters\n",
        "        q = f\"xref:Pfam-{domain}\" # The base query string for Uniprot API\n",
        "        if organism_id:\n",
        "            q += f\" AND organism_id:{organism_id}\" # Appending organism restriction\n",
        "\n",
        "        if max_proteins == 0: # If there are no proteins stop processing here\n",
        "            return [] # Return an empty list\n",
        "\n",
        "        params = {\"query\": q, \"fields\": \"accession\", \"size\": 500} # Params are set here for querying\n",
        "        ids, cursor = [], None # An empty list is set\n",
        "\n",
        "        while True: # Infinite loop, fetches result pages until no more results\n",
        "            if cursor:\n",
        "                params[\"cursor\"] = cursor # If we have cursor value from previous page, add it to request parameters. Uniprot knows where to continue from\n",
        "            r = requests.get(self.SEARCH, params=params) # Send the request to UniProt API\n",
        "            if r.status_code != 200: # Check the HTTP response, if not 200, raise an error, something on server broke or request bad\n",
        "                raise RuntimeError(f\"UniProt search failed: {r.status_code}\")\n",
        "\n",
        "            ids.extend(hit[\"primaryAccession\"] for hit in r.json()[\"results\"])# In JSON response, extract primaryAccession field and add it to ids list\n",
        "\n",
        "            if max_proteins and len(ids) >= max_proteins:# Check if enough results\n",
        "                return ids[:max_proteins] # If yes, truncate list and exit\n",
        "\n",
        "            cursor = r.json().get(\"nextCursor\") # Look in JSON in field called nextCursor, if there continue paginating\n",
        "            if not cursor: # If not present, break, since we are done\n",
        "                break\n",
        "\n",
        "        # ----- guards after pagination -----\n",
        "        if not ids: # If no ids, the protein list is empty and no proteins were found\n",
        "            raise RuntimeError(f\"No hits for {domain} (organism={organism_id})\") # Raise error, no valid data\n",
        "        return ids[:max_proteins] if max_proteins else ids # If there were ids, return all found accessions\n",
        "\n",
        "\n",
        "    def fetch_fasta(self, accession):\n",
        "        r = requests.get(f\"{self.FASTA}{accession}.fasta\") # Build url and send the HTTP GET request\n",
        "        if r.status_code != 200: # If no errors then skip\n",
        "            raise RuntimeError(f\"UniProt efetch {accession}: {r.status_code}\") # If other than 200, raise an error\n",
        "        hdr, *seq = r.text.rstrip().split(\"\\n\") # Splits the text on newline and removes the whitespaces\n",
        "        return \"\\n\".join([hdr, \"\".join(s.replace('-', '') for s in seq)]) # return the FASTA with all _ characters removed\n",
        "\n",
        "#===============================================================Implementing the CDDDatabase Version==============================================================================================\n",
        "\n",
        "# CDD database is sensitive to time and does not allow too many queries from the same IP Address in a small timeframe, so an API Key may be needed.\n",
        "\n",
        "#==================================================================================================================================================================================================\n",
        "\n",
        "NCBI_EXTRAS = {\"tool\": \"ProteinBatchDownloader\", \"email\": \"me@example.com\"}  # add api_key for a faster querying rate\n",
        "class CDDDatabase(ProteinDatabase):\n",
        "    def search_accessions(self, domain, organism_id=None, max_proteins=None): # Setting parameters\n",
        "        term = domain + (f\" AND txid{organism_id}[Organism:exp]\" if organism_id else \"\") # If the organism suffix is provided it will add it\n",
        "\n",
        "        if max_proteins == 0: # If max proteins requested is 0 then truncate right now and return an empty list\n",
        "            return []\n",
        "\n",
        "        retmax, retstart, cdd_ids = 10000, 0, [] # retmax is number if results to request per batch, retstart is to start undex for first batch, and cc_ids to collect cdd_ids from all pages\n",
        "\n",
        "        # --- eSearch pagination ---\n",
        "        while True: # A loop that runs until truncated\n",
        "            r = requests.get(# This sends a GET request to NCBI E-utilities esearch endpoint\n",
        "\n",
        "      # NCBI_EXTRAS → Contains required metadata like tool name and email, to comply with NCBI policies.\n",
        "\n",
        "       # \"db\": \"cdd\" → Targeting the CDD (Conserved Domain Database).\n",
        "\n",
        "       # \"term\": term → The search query, e.g., \"cd00184 AND txid9606[Organism:exp]\".\n",
        "\n",
        "     #   \"retmax\": retmax → Number of results to retrieve in this batch (up to 10,000).\n",
        "\n",
        "       # \"retstart\": retstart → Start offset (which record to start from).\n",
        "\n",
        "      #  \"retmode\": \"xml\" → Request XML format for easier parsing.\n",
        "\n",
        "\n",
        "                \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\",\n",
        "                params={**NCBI_EXTRAS, \"db\": \"cdd\", \"term\": term,\n",
        "                        \"retmax\": retmax, \"retstart\": retstart, \"retmode\": \"xml\"}\n",
        "    # ET.fromstring(r.text)\n",
        "    # Parses the XML text returned by NCBI into a tree structure.\n",
        "\n",
        "    # .findall(\".//Id\")\n",
        "    # Finds all <Id> elements in the XML tree (each <Id> represents a single CDD hit).\n",
        "\n",
        "    # [x.text for x in ...]\n",
        "    # Extracts the actual ID numbers as plain strings from those XML elements.\n",
        "\n",
        "    # batch\n",
        "    # This is a list of CDD IDs found in the current batch/page.\n",
        "\n",
        "\n",
        "            )\n",
        "            if r.status_code != 200: # If there was an error and the value is not 200\n",
        "                raise RuntimeError(\"CDD esearch failed\") # Raise an error\n",
        "\n",
        "            batch = [x.text for x in ET.fromstring(r.text).findall(\".//Id\")]\n",
        "            if not batch: # If no IDs found, then break since we are at the end\n",
        "                break\n",
        "            cdd_ids.extend(batch) # Extend all IDs from this batch into the master list cdd_ids\n",
        "\n",
        "            if max_proteins and len(cdd_ids) >= max_proteins: # If we collected enough then break\n",
        "                break\n",
        "            retstart += retmax # moving starting offset forward by retmax\n",
        "\n",
        "        # --- map CDD → protein -----------------------------\n",
        "        prot_ids = [] # Will store protein IDs\n",
        "        for chunk in (cdd_ids[i:i+500] for i in range(0, len(cdd_ids), 500)): # Loop over cdd_ids in chunks of 500 IDs at a time since elink cannot handle too many IDs in one go\n",
        "            r = requests.get( # Make the call to the server\n",
        "                \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/elink.fcgi\",\n",
        "                params={**NCBI_EXTRAS, \"dbfrom\": \"cdd\", \"db\": \"protein\",\n",
        "                        \"id\": \",\".join(chunk), \"retmode\": \"xml\"}\n",
        "            )\n",
        "            if r.status_code != 200: # Check the status, if there is an error or not\n",
        "                raise RuntimeError(\"CDD elink failed\")\n",
        "            prot_ids.extend(x.text for x in ET.fromstring(r.text).findall(\".//LinkSetDb/Link/Id\")) # Extract the protein ids from the returned XML and extract ids and add to prot_ids\n",
        "\n",
        "            if max_proteins and len(prot_ids) >= max_proteins: # If reached max proteins requeted then break\n",
        "                break\n",
        "\n",
        "        # ----- guards -----\n",
        "        if not prot_ids: # If there are no proteins found\n",
        "            raise RuntimeError(f\"No hits for {domain} (organism={organism_id})\") # Quit and raise an error\n",
        "        return prot_ids[:max_proteins] if max_proteins else prot_ids # Otherwise return the prot_ids\n",
        "\n",
        "    def fetch_fasta(self, accession): # A method to get FASTA swquence for one protein ID\n",
        "        r = requests.get( # Send the request to NCBI\n",
        "            \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\",\n",
        "            params={**NCBI_EXTRAS, \"db\": \"protein\", \"id\": accession, \"rettype\": \"fasta\", \"retmode\": \"text\"}\n",
        "        )\n",
        "        if r.status_code != 200: # Check if the request succeeded\n",
        "            raise RuntimeError(f\"NCBI efetch {accession}: {r.status_code}\")\n",
        "        hdr, *seq = r.text.rstrip().split(\"\\n\") # Split text\n",
        "        return \"\\n\".join([hdr, \"\".join(s.replace('-', '') for s in seq)]) # Clean and join to one string\n",
        "\n",
        "#===============================================================Implementing the PDBDatabase Version========================================================================================\n",
        "\n",
        "# Allows the user to define the organism they are interested in.\n",
        "\n",
        "#===========================================================================================================================================================================================\n",
        "class PDBDatabase(ProteinDatabase):\n",
        "    SEARCH = \"https://search.rcsb.org/rcsbsearch/v2/query\" # The URL to use for the PDB\n",
        "\n",
        "    def search_accessions(self, domain, organism_id=None, max_proteins=None): # Use this search with these parameters\n",
        "        if max_proteins == 0: # If no proteins requested then stop computing and return an empty list\n",
        "            return []\n",
        "\n",
        "        domain_node = { # a dictionary for everything we are supposed to receive\n",
        "            \"type\": \"terminal\", \"service\": \"text\",\n",
        "            \"parameters\": {\n",
        "                \"attribute\": \"rcsb_polymer_entity_annotation.annotation_id\",\n",
        "                \"operator\":  \"exact_match\",\n",
        "                \"value\":     domain\n",
        "            }\n",
        "        }\n",
        "\n",
        "        if organism_id is not None: # If the organism is defined this dictionary must be used as well\n",
        "            tax_node = {\n",
        "                \"type\": \"terminal\", \"service\": \"text\",\n",
        "                \"parameters\": {\n",
        "                    \"attribute\": \"rcsb_entity_source_organism.taxonomy_lineage.id\",\n",
        "                    \"operator\":  \"exact_match\",\n",
        "                    \"value\":     str(organism_id)\n",
        "                }\n",
        "            }\n",
        "            query = {\"type\": \"group\", \"logical_operator\": \"and\", # the query dictionary\n",
        "                     \"nodes\": [domain_node, tax_node]}\n",
        "        else:\n",
        "            query = domain_node # The query dictionary if organism not defined\n",
        "\n",
        "        hits, start, rows = [], 0, 1000 # start an empty list for results and start and row to manage chunked downloads\n",
        "        while True: # To collect hits\n",
        "            payload = { # Dictionary to handle the payload\n",
        "                \"query\": query,\n",
        "                \"return_type\": \"polymer_entity\",\n",
        "                \"request_options\": {\"paginate\": {\"start\": start, \"rows\": rows}}\n",
        "            }\n",
        "            r = requests.post(self.SEARCH, json=payload) # The JSON we get from requests\n",
        "            if r.status_code != 200: # If there is an error or not, if there us raise a run time error\n",
        "                raise RuntimeError(f\"PDB search failed: {r.status_code} {r.text}\")\n",
        "\n",
        "            chunk = [h[\"identifier\"] for h in r.json().get(\"result_set\", [])] # If we have a chunk from the parsed JSON we break it here\n",
        "            if not chunk: # If no chunk we are done paginating\n",
        "                break\n",
        "            hits.extend(chunk) # add this chunk of IDs to growing list of hits\n",
        "            if max_proteins and len(hits) >= max_proteins: # Stop early if have at least max_proteins\n",
        "                break\n",
        "            start += rows # Move onto the next page\n",
        "\n",
        "        entry_ids = list({h.split(\"_\")[0] for h in hits}) # We get things like 1ABC_1 and we break and deduplicate to get things like 1ABC\n",
        "\n",
        "        # ----- guards -----\n",
        "        if not entry_ids: # If no entries after a search, raise an error\n",
        "            raise RuntimeError(f\"No hits for {domain} (organism={organism_id})\")\n",
        "        return entry_ids[:max_proteins] if max_proteins else entry_ids # If max proteins set return after reaching it\n",
        "\n",
        "    # --------------------------------------------------------------------------\n",
        "    def fetch_fasta(self, accession, organism_id=None):\n",
        "        \"\"\"Download one PDB entry FASTA; optionally keep only chains\n",
        "        whose header mentions the organism_id substring.\"\"\"\n",
        "        r = requests.get(f\"https://www.rcsb.org/fasta/entry/{accession}\") # use this to call for FASTA\n",
        "        if r.status_code != 200: # Check if we get an error\n",
        "            raise RuntimeError(f\"PDB efetch {accession}: {r.status_code}\")\n",
        "\n",
        "        fasta = r.text # Store FASTA text body in fasta\n",
        "        if organism_id is not None: # If organism specified\n",
        "            blocks = fasta.strip().split(\"\\n>\") # Split the fasta on this term because FASTA starts with >\n",
        "            fasta = \"\\n\".join(  # Clean and join the fasta\n",
        "                (b if b.startswith(\">\") else \">\" + b)\n",
        "                for b in blocks\n",
        "                if f\"({organism_id})\" in b.split(\"\\n\", 1)[0]\n",
        "            )\n",
        "\n",
        "        if \">\" not in fasta: # no > headers left after filtering, error out\n",
        "            raise RuntimeError(\n",
        "                f\"No chains for organism {organism_id} found in {accession}\"\n",
        "            )\n",
        "\n",
        "        lines   = fasta.split(\"\\n\") # Spluit into individual lines\n",
        "        cleaned = [lines[0]] + [l.replace(\"-\", \"\") for l in lines[1:]] # Cleaned text\n",
        "        return \"\\n\".join(cleaned) # Return the joined and cleaned text\n",
        "\n",
        "#===============================================================Orchestrator Function==============================================================================================\n",
        "\n",
        "# This function brings together the functions that have already been defined earlier in the file. It is pivotal to running the program.\n",
        "\n",
        "#==================================================================================================================================================================================\n",
        "def fetch_domain_proteins_fasta(db, domain, organism_id, out_file, max_proteins, retry_attempts, pause):\n",
        "    ids = db.search_accessions(domain, organism_id, max_proteins) # Takes ids from the functions above\n",
        "    if not ids: raise RuntimeError(\"No hits\") # If no ids raises runtime error\n",
        "    with open(out_file, \"w\") as fh: # Opens a file to write\n",
        "        for acc in tqdm(ids, desc=db.__class__.__name__): # Loop over each accession id and show a progress bar with DB class name\n",
        "            for a in range(retry_attempts): # try each accession up to retry attempt\n",
        "                try:\n",
        "                    fh.write(db.fetch_fasta(acc) + \"\\n\") # Adding to a file these names\n",
        "                    break # Then break\n",
        "                except RuntimeError as e: # If there is an error\n",
        "                    if \"429\" in str(e) and a < retry_attempts - 1: # Check if rate limit error, too many requests, if so check if retries remain, and if they do sleep and try again\n",
        "                        time.sleep(2 ** a)\n",
        "                    else:\n",
        "                        raise # else raise an error\n",
        "            time.sleep(pause) # after each accession sleep to avoid hammering the server\n",
        "\n",
        "def run_pipeline(db_name, domain, organism_id=None, out_file=\"output.fasta\", max_proteins=None):\n",
        "    if os.path.exists(out_file): # If a file exists where we are trying to write then do not overwrite\n",
        "      raise FileExistsError(f\"{out_file} already exists – will not overwrite.\")\n",
        "    db = {\"uniprot\": UniProtDatabase, \"cdd\": CDDDatabase, \"pdb\": PDBDatabase}.get(db_name.lower()) # dictionary defined to look things up\n",
        "    if not db: # if no valid db then get a value error\n",
        "        raise ValueError(\"bad db\")\n",
        "    fetch_domain_proteins_fasta(db(), domain, organism_id, out_file, max_proteins, 3, 0.34)\n",
        "# db() creates an instance of the selected database class.\n",
        "\n",
        "# Calls fetch_domain_proteins_fasta() with:\n",
        "\n",
        "#     db() → the DB instance\n",
        "\n",
        "#     domain, organism_id, out_file, max_proteins → passed as arguments\n",
        "\n",
        "#     3 → retry attempts\n",
        "\n",
        "#     0.34 → pause after each accession\n",
        "\n",
        "    print(f\"{db_name}: wrote {out_file}\") # log success\n",
        "\n",
        "#==============================================================================Main Function==================================================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    for db in (\"uniprot\", \"cdd\", \"pdb\"):\n",
        "        run_pipeline(db, \"PF00018\" if db != \"cdd\" else \"cd00184\", None, f\"{db}.fasta\", 5)\n",
        "\n",
        "\n",
        "\n",
        "# db: database name string (\"uniprot\", \"cdd\", or \"pdb\")\n",
        "\n",
        "# domain: as discussed above (Pfam or CDD ID) -> Important and where ID is added\n",
        "\n",
        "# organism_id: None, meaning all organisms -> Important and where organism is specified\n",
        "\n",
        "# out_file: filename string, e.g., \"uniprot.fasta\"\n",
        "\n",
        "# max_proteins: 5 -> Important and where proteins are specified\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SRL0KM7VvZF3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
